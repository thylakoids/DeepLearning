{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[有道word2vec](pdf/word2vec.pdf)\n",
    "\n",
    "[An Anatomy of Key Tricks in word2vec project with examples](http://nbviewer.jupyter.org/github/dolaameng/tutorials/blob/master/word2vec-abc/poc/pyword2vec_anatomy.ipynb)\n",
    "\n",
    "[word2vec傻瓜剖析](http://xiaoquanzi.net/?p=156)\n",
    "\n",
    "[cs224N/Ling](https://www.youtube.com/watch?v=ERibwqs9p38)\n",
    "\n",
    "### Motivation\n",
    "- one-hot vector has no inherent notion of similarity\n",
    "- Distributional similarity:\n",
    "    - using words in the context to represent the meanning \n",
    "    > *You shall know a word by the company it keeps.*\n",
    "                                   J.R.Firth\n",
    "\n",
    "### Main idea \n",
    "- Predict between word and its context words! \n",
    "- Two algorithms:\n",
    "    1. **Skip-Grams (SG)**\n",
    "            predict context words given target\n",
    "    2. Continus Bag of Words (CBOW)\n",
    "            predict target word given bag-of-words context\n",
    "- Two traing algotithms:\n",
    "    1. Hierachical softmax\n",
    "    2. Negtive sampling\n",
    "    \n",
    "    **Naive softmax**\n",
    "    \n",
    "    \n",
    "- Two vector for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
