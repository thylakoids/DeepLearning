{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU (Rectified Linear Units) Layers\n",
    "\n",
    "After each conv layer, it is convention to apply a nonlinear layer (or activation layer) immediately afterward.The purpose of this layer is to introduce nonlinearity to a system that basically has just been computing linear operations during the conv layers (just element wise multiplications and summations).In the past, nonlinear functions like tanh and sigmoid were used, but researchers found out that ReLU layers work far better because the network is able to train a lot faster (because of the computational efficiency) without making a significant difference to the accuracy. It also helps to alleviate the vanishing gradient problem, which is the issue where the lower layers of the network train very slowly because the gradient decreases exponentially through the layers. The ReLU layer applies the function f(x) = max(0, x) to all of the values in the input volume. In basic terms, this layer just changes all the negative activations to 0.This layer increases the nonlinear properties of the model and the overall network without affecting the receptive fields of the conv layer.\n",
    "\n",
    "[Paper](http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf) by the great Geoffrey Hinton (aka the father of deep learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "The function of the pooling layer is to progressively reduce the spatial size of the representation to **reduce the amount of parameters and computation** in the network, and hence to also **control overfitting**. No learning takes place on the pooling layers.\n",
    "![MaxPool](img/MaxPool.png)\n",
    "Pooling units are obtained using functions like **max-pooling**, **average pooling** and even **L2-norm pooling**.\n",
    "![max-average-pooling](img/max-average-pooling.png)\n",
    "Max pooling extracts the most important features like edges whereas, average pooling extracts features so smoothly. For image data, you can see the difference. Although both are used for same reason, I think max pooling is better for extracting the extreme features. Average pooling sometimes can’t extract good features because it takes all into count and results an average value which may/may not be important for object detection type tasks.\n",
    "\n",
    "[max vs average](https://www.quora.com/What-is-the-impact-of-different-pooling-methods-in-convolutional-neural-networks-Are-there-any-papers-that-compare-justify-different-pooling-strategies-max-pooling-average-etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layers\n",
    "Now, dropout layers have a very specific function in neural networks. In the last section, we discussed the problem of overfitting, where after training, the weights of the network are so tuned to the training examples they are given that the network doesn’t perform well when given new examples. The idea of dropout is simplistic in nature. This layer “drops out” a random set of activations in that layer by setting them to zero. Simple as that. Now, what are the benefits of such a simple and seemingly unnecessary and counterintuitive process? Well, in a way, it forces the network to be redundant. By that I mean the network should be able to provide the right classification or output for a specific example even if some of the activations are dropped out. It makes sure that the network isn’t getting too “fitted” to the training data and thus helps alleviate the overfitting problem. An important note is that this layer is only used during training, and not during test time.\n",
    "\n",
    "[Paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) by Geoffrey Hinton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network in Network Layers\n",
    "![NIN1](img/NIN1.png)\n",
    "\n",
    "![NIN](img/NIN2.png)\n",
    "\n",
    "mlpconv=convolution+1×1convolution+1×1convolution\n",
    "\n",
    "A network in network layer refers to a conv layer where a 1 x 1 size filter is used. Now, at first look, you might wonder why this type of layer would even be helpful since receptive fields are normally larger than the space they map to. However, we must remember that these 1x1 convolutions span a certain depth, so we can think of it as a 1 x 1 x N convolution where N is the number of filters applied in the layer. Effectively, this layer is performing a N-D element-wise multiplication where N is the depth of the input volume into the layer.\n",
    "![NiN](img/NetinNet.png)\n",
    "\n",
    "[C4W2L05 Network in Network](https://www.youtube.com/watch?v=c1RBQzKsDCk)\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/1312.4400v3.pdf) by Min Lin.([local](pdf/13_network in net work.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Network\n",
    "- Why choose? Use all parallel!\n",
    "- Using 1 X 1 convolution (network in network) to save compution cost.\n",
    "![GoogleNet](img/GoogleNet.png)\n",
    "![Inception](img/Inception.png)\n",
    "![111](img/111.png)\n",
    "![112](img/112.png)\n",
    "![Branch](img/Branch.png)\n",
    "![name](img/name.png)\n",
    "\n",
    "[C4W2L06 Inception Network](https://www.youtube.com/watch?v=C86ZXvgpejM)(Video)\n",
    "\n",
    "[C4W2L07 Inception Network](https://www.youtube.com/watch?v=KfV8CJh7hE0)(Video)\n",
    "\n",
    "[Paper:Going Deeper with Convolutions](pdf/2015_GoogleNet.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Referrence\n",
    "[A Beginner's Guide To Understanding Convolutional Neural Networks ](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)(Post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
